{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using parallelformers To parallelize models form Huggingface\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, pipeline\n",
    "from torch.distributed.pipeline.sync import Pipe\n",
    "import torch\n",
    "import os\n",
    "from parallelformers import parallelize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "dtype = torch.float\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\", cache_dir='/root/NAS/huggingface_cache', torch_dtype=dtype, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelize(model, num_gpus=2, fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"How babies are born?\"]*1000\n",
    "\n",
    "for inp in tqdm(text):\n",
    "    generate_kwargs = {'max_length': 300}\n",
    "\n",
    "    input_ids = tokenizer(inp, return_tensors=\"pt\").to('cuda')\n",
    "    outs = model.generate(**input_ids, **generate_kwargs).cpu()\n",
    "\n",
    "    aa = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device, torch_dtype=dtype)\n",
    "generator(text, **generate_kwargs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Consumption:\n",
    "# 1. Size of moedel\n",
    "# 2. Intermediate outputs during forward pass (Depend on batch size)\n",
    "# 3. Discard intermididte values and store gradient values in backword pass (Equal to Model Size)\n",
    "# 4. Store all the states for optimizer for each weight parameter in optim step (n_states x Model size)\n",
    "\n",
    "# 5. Mixed Precision Training cuts the memory in half during orward pass by using 16bit for forward pass and storing gradients in 32bits\n",
    "# 6. Twice the memory to store the model when using DDP (Need to store gradients communicated from all other GPUs)\n",
    "# 7. During Infrence memeory consumed is only model size\n",
    "\n",
    "\n",
    "\n",
    "d_model=4096\n",
    "nhead=64\n",
    "num_encoder_layers=8\n",
    "num_decoder_layers=4\n",
    "dim_feedforward=10240\n",
    "dim_feedforward=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Paralleism on 2 GPUs\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        en_layer = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        den_layer = torch.nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "\n",
    "        self.enc = nn.Sequential(nn.TransformerEncoder(en_layer, num_encoder_layers)).to(0)\n",
    "\n",
    "        self.dec = nn.TransformerDecoder(den_layer, num_decoder_layers).to(1)\n",
    "\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \n",
    "        memory = self.enc(src.to(0))\n",
    "        \n",
    "        out = self.dec(tgt.to(1), memory.to(1))\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = Transformer()\n",
    "print(f'Size of model is: {sum(p.numel() for p in model.parameters() if p.requires_grad)*4 :,} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cretation = nn.MSELoss()\n",
    "optim = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "in_seq_len = 12\n",
    "out_seq_len = 18\n",
    "batch_size = 128\n",
    "embedding_size = d_model\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(10)):\n",
    "    src = torch.randn(in_seq_len, batch_size, embedding_size)\n",
    "    tgt = torch.randn(out_seq_len, batch_size, embedding_size)\n",
    "\n",
    "    # Forward Pass\n",
    "    optim.zero_grad()\n",
    "    out = model(src, tgt[:-1, :, :])\n",
    "    loss = cretation(out, tgt[1:, :].to(1))\n",
    "    # Backword Pass\n",
    "    loss.backward()\n",
    "    # Optim step\n",
    "    optim.step()\n",
    "\n",
    "print(f\"Time taken for Model parallel on 2 GPUs: {time.time() - start :0.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from varuna import CutPoint, Varuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Paralleism on 2 GPUs with Varuna\n",
    "\n",
    "def get_batch_fn(size: int, device: torch.device):\n",
    "    in_seq_len = 12\n",
    "    out_seq_len = 18\n",
    "\n",
    "    batch_size = size\n",
    "\n",
    "    embedding_size = d_model\n",
    "    src = torch.randn(batch_size, in_seq_len, embedding_size).to(device)\n",
    "    tgt = torch.randn(batch_size, out_seq_len, embedding_size).to(device)\n",
    "\n",
    "    return {\"src\": src, \"tgt\": tgt}\n",
    "\n",
    "class TransformerVaruna(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerVaruna, self).__init__()\n",
    "\n",
    "        en_layer = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        den_layer = torch.nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "\n",
    "        num_cutpoints = 1\n",
    "        self.cutpoints = [CutPoint() for i in range(num_cutpoints)]\n",
    "\n",
    "        self.enc = nn.Sequential(nn.TransformerEncoder(en_layer, num_encoder_layers))\n",
    "        self.dec = nn.TransformerDecoder(den_layer, num_decoder_layers)\n",
    "\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \n",
    "        memory = self.enc(src)\n",
    "        memory = self.cutpoints[0](memory)\n",
    "\n",
    "        out = self.dec(tgt, memory)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = TransformerVaruna()\n",
    "print(f'Size of model is: {sum(p.numel() for p in model.parameters() if p.requires_grad)*4 :,} bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Varuna(model, args.stage_to_rank_map, get_batch_fn, args.batch_size, args.chunk_size, args.fp16, local_rank=args.local_rank, device=args.local_rank)\n",
    "\n",
    "cretation = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "model.set_optimizer(optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(10):\n",
    "    inputs = get_batch_fn(100, 'cpu')\n",
    "    loss, overflow = model.step(inputs)\n",
    "    loss = torch.Tensor([loss])\n",
    "\n",
    "    if not overflow:\n",
    "        optimizer.step()\n",
    "print(f\"Time taken for Model parallel on 2 GPUs: {time.time() - start :0.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
